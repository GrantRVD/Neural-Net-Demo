{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Demo\n",
    "\n",
    "By Grant R. Vousden-Dishington\n",
    "\n",
    "This notebook is heavily based on two blogs **A Neural Network in 11 lines of Python** ([Part 1](http://iamtrask.github.io/2015/07/12/basic-python-network/) & [Part 2](http://iamtrask.github.io/2015/07/27/python-network-part2/)) and **Hinton's Droptout in 3 lines of Python**, both by [*@iamtrask*](iamtrask.github.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function: \"squashes\" numbers into probabilities\n",
    "def nonlin(x, deriv=False):\n",
    "    x = x * (1-x) if deriv else 1 / (1 + np.exp(-x))\n",
    "    return x\n",
    "\n",
    "# Input: 4 training examples, 3-dimensional (4 x 3)\n",
    "nnInput = np.array([\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 1],\n",
    "        [1, 0, 1],\n",
    "        [1, 1, 1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training\n",
      "[[ 0.00390254]\n",
      " [ 0.00318154]\n",
      " [ 0.99740413]\n",
      " [ 0.99681544]]\n"
     ]
    }
   ],
   "source": [
    "# Output: expected classifications (4 x 1)\n",
    "nnOutput2 = np.array([[0, 0, 1 ,1]]).T\n",
    "\n",
    "# Initialize weights with mean of 0, seed random generator\n",
    "np.random.seed(261015)\n",
    "syn0 = 2 * np.random.random((3, 1)) - 1  # (3 x 1)\n",
    "\n",
    "# Do full-batch training\n",
    "for i in range(60000):\n",
    "    \n",
    "    # Forward propagation\n",
    "    l0 = nnInput\n",
    "    l1 = nonlin(l0 @ syn0)\n",
    "    \n",
    "    # Error: expected minus predicted (4 x 1)\n",
    "    errl1 = nnOutput2 - l1\n",
    "    \n",
    "    # Calculate delta: the error times slope of the sigmoid at l1 values\n",
    "    deltal1 = errl1 * nonlin(l1, deriv=True)\n",
    "    \n",
    "    # Update weights: matrix multiply the input times the delta values all at once\n",
    "    syn0 += l0.T @ deltal1\n",
    "    \n",
    "print(\"Output after training\")\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer back-propagating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.500646983197\n",
      "Error: 0.0108313939135\n",
      "Error: 0.00721546010071\n",
      "Error: 0.00571646037978\n",
      "Error: 0.00485302997199\n",
      "Error: 0.00427728962702\n"
     ]
    }
   ],
   "source": [
    "# Output: expected classifications (4 x 1)\n",
    "nnOutput3 = np.array([[0, 1, 1 ,0]]).T\n",
    "\n",
    "# Initialize weights with mean of 0, seed random generator\n",
    "np.random.seed(261015)\n",
    "syn0 = 2 * np.random.random((3, 4)) - 1  # (3 x 4)\n",
    "syn1 = 2 * np.random.random((4, 1)) - 1  # (4 x 1)\n",
    "\n",
    "# Do full-batch learning\n",
    "for i in range(60000):\n",
    "    \n",
    "    # Forward Propagation\n",
    "    l0 = nnInput\n",
    "    l1 = nonlin(l0 @ syn0)\n",
    "    l2 = nonlin(l1 @ syn1)\n",
    "    \n",
    "    # Output error: expected minus predicted (4 x 1)\n",
    "    errl2 = nnOutput3 - l2\n",
    "    \n",
    "    # Getting a running view of the error every 10000 steps\n",
    "    if not i % 10000:\n",
    "        print(\"Error: \" + str(np.mean(np.abs(errl2))))\n",
    "        \n",
    "    # Calculate delta for output layer: the error times slope of the sigmoid at l2 values (4 x 1)\n",
    "    deltal2 = errl2 * nonlin(l2, deriv=True)\n",
    "    \n",
    "    # Hidden error: how much hidden layer contributed to output error (4 x 4)\n",
    "    # THIS IS THE BACK-PROPAGATION STEP\n",
    "    errl1 = deltal2 @ syn1.T\n",
    "    \n",
    "    # Calculate delta for hidden layer: the error times the sigmoid at l2 values (4 x 4)\n",
    "    deltal1 = errl1 * nonlin(l1, deriv=True)\n",
    "    \n",
    "    # Update weights\n",
    "    syn0 += l0.T @ deltal1\n",
    "    syn1 += l1.T @ deltal2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer back-propagating network Mk. 2: Gradient Descent (alpha tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with α 0.001\n",
      "Error: 0.496410031903\n",
      "Error: 0.495164025493\n",
      "Error: 0.493596043188\n",
      "Error: 0.491606358559\n",
      "Error: 0.489100166544\n",
      "Error: 0.485977857846\n",
      "Syn0\n",
      "[[-0.28448441  0.32471214 -1.53496167 -0.47594822]\n",
      " [-0.7550616  -1.04593014 -1.45446052 -0.32606771]\n",
      " [-0.2594825  -0.13487028 -0.29722666  0.40028038]]\n",
      "Syn0 direction changes\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  1.]]\n",
      "Syn1\n",
      "[[-0.61957526]\n",
      " [ 0.76414675]\n",
      " [-1.49797046]\n",
      " [ 0.40734574]]\n",
      "Syn1 direction changes\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "Training with α 0.01\n",
      "Error: 0.496410031903\n",
      "Error: 0.457431074442\n",
      "Error: 0.359097202563\n",
      "Error: 0.239358137159\n",
      "Error: 0.143070659013\n",
      "Error: 0.0985964298089\n",
      "Syn0\n",
      "[[ 2.39225985  2.56885428 -5.38289334 -3.29231397]\n",
      " [-0.35379718 -4.6509363  -5.67005693 -1.74287864]\n",
      " [-0.15431323 -1.17147894  1.97979367  3.44633281]]\n",
      "Syn0 direction changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 2.  0.  0.  2.]\n",
      " [ 4.  2.  1.  1.]]\n",
      "Syn1\n",
      "[[-3.70045078]\n",
      " [ 4.57578637]\n",
      " [-7.63362462]\n",
      " [ 4.73787613]]\n",
      "Syn1 direction changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "Training with α 0.1\n",
      "Error: 0.496410031903\n",
      "Error: 0.0428880170001\n",
      "Error: 0.0240989942285\n",
      "Error: 0.0181106521468\n",
      "Error: 0.0149876162722\n",
      "Error: 0.0130144905381\n",
      "Syn0\n",
      "[[ 3.88035459  3.6391263  -5.99509098 -3.8224267 ]\n",
      " [-1.72462557 -5.41496387 -6.30737281 -3.03987763]\n",
      " [ 0.45953952 -1.77301389  2.37235987  5.04309824]]\n",
      "Syn0 direction changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 2.  0.  0.  2.]\n",
      " [ 4.  2.  1.  1.]]\n",
      "Syn1\n",
      "[[-5.72386389]\n",
      " [ 6.15041318]\n",
      " [-9.40272079]\n",
      " [ 6.61461026]]\n",
      "Syn1 direction changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "Training with α 1\n",
      "Error: 0.496410031903\n",
      "Error: 0.00858452565325\n",
      "Error: 0.00578945986251\n",
      "Error: 0.00462917677677\n",
      "Error: 0.00395876528027\n",
      "Error: 0.00351012256786\n",
      "Syn0\n",
      "[[ 4.6013571   4.17197193 -6.30956245 -4.19745118]\n",
      " [-2.58413484 -5.81447929 -6.60793435 -3.68396123]\n",
      " [ 0.97538679 -2.02685775  2.52949751  5.84371739]]\n",
      "Syn0 direction changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 2.  0.  0.  2.]\n",
      " [ 4.  2.  1.  1.]]\n",
      "Syn1\n",
      "[[ -6.96765763]\n",
      " [  7.14101949]\n",
      " [-10.31917382]\n",
      " [  7.86128405]]\n",
      "Syn1 direction changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "Training with α 10\n",
      "Error: 0.496410031903\n",
      "Error: 0.00312938876301\n",
      "Error: 0.00214459557985\n",
      "Error: 0.00172397549956\n",
      "Error: 0.00147821451229\n",
      "Error: 0.00131274062834\n",
      "Syn0\n",
      "[[ 4.52597806  5.77663165 -7.34266481 -5.29379829]\n",
      " [ 1.66715206 -7.16447274 -7.99779235 -1.81881849]\n",
      " [-4.27032921 -3.35838279  3.44594007  4.88852208]]\n",
      "Syn0 direction changes\n",
      "[[  7.  19.   2.   6.]\n",
      " [  7.   2.   0.  22.]\n",
      " [ 19.  26.   9.  17.]]\n",
      "Syn1\n",
      "[[ -8.58485788]\n",
      " [ 10.1786297 ]\n",
      " [-14.87601886]\n",
      " [  7.57026121]]\n",
      "Syn1 direction changes\n",
      "[[ 22.]\n",
      " [ 15.]\n",
      " [  4.]\n",
      " [ 15.]]\n",
      "Training with α 100\n",
      "Error: 0.496410031903\n",
      "Error: 0.125476983855\n",
      "Error: 0.125330333528\n",
      "Error: 0.125267728765\n",
      "Error: 0.12523107366\n",
      "Error: 0.125206352756\n",
      "Syn0\n",
      "[[-17.20515374   1.89881432 -16.95533155  -8.23482697]\n",
      " [  5.70240659 -17.23785161  -9.48052574  -7.92972576]\n",
      " [ -4.18781704  -0.3388181    2.82024759  -8.40059859]]\n",
      "Syn0 direction changes\n",
      "[[  8.   7.   3.   2.]\n",
      " [ 13.   8.   2.   4.]\n",
      " [ 16.  13.  12.   8.]]\n",
      "Syn1\n",
      "[[  9.68285369]\n",
      " [  9.55731916]\n",
      " [-16.0390702 ]\n",
      " [  6.27326973]]\n",
      "Syn1 direction changes\n",
      "[[ 13.]\n",
      " [ 11.]\n",
      " [ 12.]\n",
      " [ 10.]]\n",
      "Training with α 1000\n",
      "Error: 0.496410031903\n",
      "Error: 0.5\n",
      "Error: 0.5\n",
      "Error: 0.5\n",
      "Error: 0.5\n",
      "Error: 0.5\n",
      "Syn0\n",
      "[[-56.06177241  -4.66409623  -5.65196179 -23.05868769]\n",
      " [ -4.52271708  -4.78184499 -10.88770202 -15.85879101]\n",
      " [-89.56678495  10.81119741  37.02351518 -48.33299795]]\n",
      "Syn0 direction changes\n",
      "[[ 3.  2.  4.  1.]\n",
      " [ 1.  2.  2.  1.]\n",
      " [ 6.  6.  4.  1.]]\n",
      "Syn1\n",
      "[[  25.16188889]\n",
      " [  -8.68235535]\n",
      " [-116.60053379]\n",
      " [  11.41582458]]\n",
      "Syn1 direction changes\n",
      "[[ 7.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 3.]]\n"
     ]
    }
   ],
   "source": [
    "# Output: expected classifications (4 x 1)\n",
    "nnOutput3 = np.array([[0, 1, 1 ,0]]).T\n",
    "\n",
    "alphas = [.001, .01, .1, 1, 10, 100, 1000]\n",
    "\n",
    "# Try training with each different alpha-value\n",
    "for a in alphas:\n",
    "    # Initialize weights with mean of 0, seed random generator\n",
    "    np.random.seed(271015)\n",
    "    syn0 = 2 * np.random.random((3, 4)) - 1  # (3 x 4)\n",
    "    syn1 = 2 * np.random.random((4, 1)) - 1  # (4 x 1)\n",
    "\n",
    "    # This time, we track the previous weights for syn0 and syn1, as well as the direction of change\n",
    "    syn0prev = np.zeros_like(syn0)\n",
    "    syn1prev = np.zeros_like(syn1)\n",
    "    syn0dir = np.zeros_like(syn0)\n",
    "    syn1dir = np.zeros_like(syn1)\n",
    "    \n",
    "    print(\"Training with \\u03B1 \" + str(a))  # 03B1 is unicode for lowercase alpha\n",
    "    \n",
    "    # Do full-batch learning\n",
    "    for i in range(60000):\n",
    "\n",
    "        # Forward Propagation\n",
    "        l0 = nnInput\n",
    "        l1 = nonlin(l0 @ syn0)\n",
    "        l2 = nonlin(l1 @ syn1)\n",
    "\n",
    "        # Output error: expected minus predicted (4 x 1)\n",
    "        errl2 = nnOutput3 - l2\n",
    "\n",
    "        # Getting a running view of the error every 10000 steps\n",
    "        if not i % 10000:\n",
    "            print(\"Error: \" + str(np.mean(np.abs(errl2))))\n",
    "\n",
    "        # Calculate delta for output layer: the error times slope of the sigmoid at l2 values (4 x 1)\n",
    "        deltal2 = errl2 * nonlin(l2, deriv=True)\n",
    "\n",
    "        # Hidden error: how much hidden layer contributed to output error (4 x 4)\n",
    "        # THIS IS THE BACK-PROPAGATION STEP\n",
    "        errl1 = deltal2 @ syn1.T\n",
    "\n",
    "        # Calculate delta for hidden layer: the error times the sigmoid at l2 values (4 x 4)\n",
    "        deltal1 = errl1 * nonlin(l1, deriv=True)\n",
    "\n",
    "        # Calculate the updates to the weights\n",
    "        syn0update = l0.T @ deltal1\n",
    "        syn1update = l1.T @ deltal2\n",
    "        \n",
    "        # For tracking, we see if the direction of the update has changed since the last step \n",
    "        syn0dir += (syn0update * syn0prev) < 0\n",
    "        syn1dir += (syn1update * syn1prev) < 0\n",
    "        \n",
    "        # Update weights: this is where the alpha values come in\n",
    "        syn0 += a * syn0update\n",
    "        syn1 += a * syn1update\n",
    "        \n",
    "        syn0prev = syn0update\n",
    "        syn1prev = syn1update\n",
    "        \n",
    "    # Print out the results for this alpha\n",
    "    print(\"Syn0\")\n",
    "    print(syn0)\n",
    "    print(\"Syn0 direction changes\")\n",
    "    print(syn0dir)\n",
    "\n",
    "    print(\"Syn1\")\n",
    "    print(syn1)\n",
    "    print(\"Syn1 direction changes\")\n",
    "    print(syn1dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer back-propagating network Mk. 3: Parameterizing the hidden layer size\n",
    "\n",
    "For this dataset, the previous excercise demonstrated that an alpha of 10 worked best, and the hidden layer size was fixed at 4 x 1. In the original iamtrask post, this step only tested the performance of a network with 32 nodes. Instead of testing just one size, we'll try several here and keep the alpha constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2 hidden layer nodes \n",
      "Error: 0.500542514772\n",
      "Error: 0.375678086516\n",
      "Error: 0.375611385993\n",
      "Error: 0.375562977412\n",
      "Error: 0.375527817871\n",
      "Error: 0.375502700424\n",
      "Training with 4 hidden layer nodes \n",
      "Error: 0.498898932827\n",
      "Error: 0.00277424125556\n",
      "Error: 0.00194289672755\n",
      "Error: 0.00157880649911\n",
      "Error: 0.00136305538713\n",
      "Error: 0.00121639933639\n",
      "Training with 8 hidden layer nodes \n",
      "Error: 0.498573920102\n",
      "Error: 0.00247201806838\n",
      "Error: 0.00170567040317\n",
      "Error: 0.00137593796584\n",
      "Error: 0.00118242179508\n",
      "Error: 0.00105172558577\n",
      "Training with 16 hidden layer nodes \n",
      "Error: 0.496319067934\n",
      "Error: 0.00230574062297\n",
      "Error: 0.00159283504815\n",
      "Error: 0.00128457712047\n",
      "Error: 0.0011032857294\n",
      "Error: 0.000980712209009\n",
      "Training with 32 hidden layer nodes \n",
      "Error: 0.493817543054\n",
      "Error: 0.00332116672913\n",
      "Error: 0.0018904703995\n",
      "Error: 0.00144658328493\n",
      "Error: 0.00121176944923\n",
      "Error: 0.00106133852859\n",
      "Training with 64 hidden layer nodes \n",
      "Error: 0.495697784403\n",
      "Error: 0.499999999843\n",
      "Error: 0.499999999842\n",
      "Error: 0.499999999842\n",
      "Error: 0.499999999841\n",
      "Error: 0.499999999841\n"
     ]
    }
   ],
   "source": [
    "# Output: expected classifications (4 x 1)\n",
    "nnOutput3 = np.array([[0, 1, 1 ,0]]).T\n",
    "\n",
    "# Set alpha and the hidden layer sizes we want to test\n",
    "a = 10\n",
    "l1sizes = [2, 4, 8, 16, 32, 64]\n",
    "\n",
    "# Try training with each different alpha-value\n",
    "for s in l1sizes:\n",
    "    # Initialize weights with mean of 0, seed random generator\n",
    "    # This is where the different hidden layer sizes come in\n",
    "    np.random.seed(271015)\n",
    "    syn0 = 2 * np.random.random((3, s)) - 1  # (3 x s)\n",
    "    syn1 = 2 * np.random.random((s, 1)) - 1  # (s x 1)\n",
    "    \n",
    "    print(\"Training with {} hidden layer nodes \".format(s))\n",
    "    \n",
    "    # Do full-batch learning\n",
    "    for i in range(60000):\n",
    "\n",
    "        # Forward Propagation\n",
    "        l0 = nnInput\n",
    "        l1 = nonlin(l0 @ syn0)\n",
    "        l2 = nonlin(l1 @ syn1)\n",
    "\n",
    "        # Output error: expected minus predicted (4 x 1)\n",
    "        errl2 = nnOutput3 - l2\n",
    "\n",
    "        # Getting a running view of the error every 10000 steps\n",
    "        if not i % 10000:\n",
    "            print(\"Error: \" + str(np.mean(np.abs(errl2))))\n",
    "\n",
    "        # Calculate delta for output layer: the error times slope of the sigmoid at l2 values (4 x 1)\n",
    "        deltal2 = errl2 * nonlin(l2, deriv=True)\n",
    "\n",
    "        # Hidden error: how much hidden layer contributed to output error (4 x 4)\n",
    "        # THIS IS THE BACK-PROPAGATION STEP\n",
    "        errl1 = deltal2 @ syn1.T\n",
    "\n",
    "        # Calculate delta for hidden layer: the error times the sigmoid at l2 values (4 x 4)\n",
    "        deltal1 = errl1 * nonlin(l1, deriv=True)\n",
    "\n",
    "        # Calculate the updates to the weights\n",
    "        syn0 += a * (l0.T @ deltal1)\n",
    "        syn1 += a * (l1.T @ deltal2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
