{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Demo\n",
    "\n",
    "By Grant R. Vousden-Dishington\n",
    "\n",
    "This notebook is heavily based on two blogs **A Neural Network in 11 lines of Python** ([Part 1](http://iamtrask.github.io/2015/07/12/basic-python-network/) & [Part 2](http://iamtrask.github.io/2015/07/27/python-network-part2/)) and [**Hinton's Droptout in 3 Lines of Python**](http://iamtrask.github.io/2015/07/28/dropout/), both by [*@iamtrask*](iamtrask.github.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid function: \"squashes\" numbers into probabilities\n",
    "def nonlin(x, deriv=False):\n",
    "    x = x * (1-x) if deriv else 1 / (1 + np.exp(-x))\n",
    "    return x\n",
    "\n",
    "# Input: 4 training examples, 3-dimensional (4 x 3)\n",
    "nnInput = np.array([\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 1],\n",
    "        [1, 0, 1],\n",
    "        [1, 1, 1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-layer feed-forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after training\n",
      "[[ 0.00390254]\n",
      " [ 0.00318154]\n",
      " [ 0.99740413]\n",
      " [ 0.99681544]]\n"
     ]
    }
   ],
   "source": [
    "# Output: expected classifications (4 x 1)\n",
    "nnOutput2 = np.array([[0, 0, 1 ,1]]).T\n",
    "\n",
    "# Initialize weights with mean of 0, seed random generator\n",
    "np.random.seed(261015)\n",
    "syn0 = 2 * np.random.random((3, 1)) - 1  # (3 x 1)\n",
    "\n",
    "# Do full-batch training\n",
    "for i in range(60000):\n",
    "    \n",
    "    # Forward propagation\n",
    "    l0 = nnInput\n",
    "    l1 = nonlin(l0 @ syn0)\n",
    "    \n",
    "    # Error: expected minus predicted (4 x 1)\n",
    "    errl1 = nnOutput2 - l1\n",
    "    \n",
    "    # Calculate delta: the error times slope of the sigmoid at l1 values\n",
    "    deltal1 = errl1 * nonlin(l1, deriv=True)\n",
    "    \n",
    "    # Update weights: matrix multiply the input times the delta values all at once\n",
    "    syn0 += l0.T @ deltal1\n",
    "    \n",
    "print(\"Output after training\")\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer back-propagating network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.500646983197\n",
      "Error: 0.0108313939135\n",
      "Error: 0.00721546010071\n",
      "Error: 0.00571646037978\n",
      "Error: 0.00485302997199\n",
      "Error: 0.00427728962702\n"
     ]
    }
   ],
   "source": [
    "# Output: expected classifications (4 x 1)\n",
    "nnOutput3 = np.array([[0, 1, 1 ,0]]).T\n",
    "\n",
    "# Initialize weights with mean of 0, seed random generator\n",
    "np.random.seed(261015)\n",
    "syn0 = 2 * np.random.random((3, 4)) - 1  # (3 x 4)\n",
    "syn1 = 2 * np.random.random((4, 1)) - 1  # (4 x 1)\n",
    "\n",
    "# Do full-batch learning\n",
    "for i in range(60000):\n",
    "    \n",
    "    # Forward Propagation\n",
    "    l0 = nnInput\n",
    "    l1 = nonlin(l0 @ syn0)\n",
    "    l2 = nonlin(l1 @ syn1)\n",
    "    \n",
    "    # Output error: expected minus predicted (4 x 1)\n",
    "    errl2 = nnOutput3 - l2\n",
    "    \n",
    "    # Getting a running view of the error every 10000 steps\n",
    "    if not i % 10000:\n",
    "        print(\"Error: \" + str(np.mean(np.abs(errl2))))\n",
    "        \n",
    "    # Calculate delta for output layer: the error times slope of the sigmoid at l2 values (4 x 1)\n",
    "    deltal2 = errl2 * nonlin(l2, deriv=True)\n",
    "    \n",
    "    # Hidden error: how much hidden layer contributed to output error (4 x 4)\n",
    "    # THIS IS THE BACK-PROPAGATION STEP\n",
    "    errl1 = deltal2 @ syn1.T\n",
    "    \n",
    "    # Calculate delta for hidden layer: the error times the sigmoid at l2 values (4 x 4)\n",
    "    deltal1 = errl1 * nonlin(l1, deriv=True)\n",
    "    \n",
    "    # Update weights\n",
    "    syn0 += l0.T @ deltal1\n",
    "    syn1 += l1.T @ deltal2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer back-propagating network Mk. 2: Gradient Descent (alpha tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with α 0.001\n",
      "Error: 0.498898932827\n",
      "Error: 0.498796948145\n",
      "Error: 0.498686565246\n",
      "Error: 0.498571609282\n",
      "Error: 0.498448841313\n",
      "Error: 0.498314805117\n",
      "Syn0\n",
      "[[ 0.58024654 -0.41866451  0.60108431 -0.67801471]\n",
      " [-0.70958078  0.19528945  0.80434397  0.37701121]\n",
      " [ 0.72880237 -0.61656649 -0.48386339 -0.36528227]]\n",
      "Syn0 direction changes\n",
      "[[ 0.  0.  1.  0.]\n",
      " [ 1.  0.  1.  1.]\n",
      " [ 0.  0.  1.  0.]]\n",
      "Syn1\n",
      "[[-0.57321094]\n",
      " [ 0.03002241]\n",
      " [ 0.03812072]\n",
      " [ 0.87185874]]\n",
      "Syn1 direction changes\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "Training with α 0.01\n",
      "Error: 0.498898932827\n",
      "Error: 0.497337070621\n",
      "Error: 0.489379108547\n",
      "Error: 0.432236465616\n",
      "Error: 0.303557999185\n",
      "Error: 0.160599355169\n",
      "Syn0\n",
      "[[ 4.66593475 -1.75009237  4.69524699 -3.57501148]\n",
      " [-3.40666199  4.64328691  3.77424503  2.36547241]\n",
      " [ 1.79534168 -0.24637092 -0.80578793 -1.40299381]]\n",
      "Syn0 direction changes\n",
      "[[ 0.  3.  1.  0.]\n",
      " [ 1.  2.  1.  1.]\n",
      " [ 1.  3.  2.  1.]]\n",
      "Syn1\n",
      "[[-3.40358457]\n",
      " [-5.13222824]\n",
      " [ 5.99505957]\n",
      " [ 3.02751128]]\n",
      "Syn1 direction changes\n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "Training with α 0.1\n",
      "Error: 0.498898932827\n",
      "Error: 0.0524997405323\n",
      "Error: 0.0289821740016\n",
      "Error: 0.0219098581773\n",
      "Error: 0.0182408457775\n",
      "Error: 0.0159189907138\n",
      "Syn0\n",
      "[[ 5.77250128 -3.25994119  5.48914149 -5.20725205]\n",
      " [-4.02326847  6.68130621  4.73335844  3.24482997]\n",
      " [ 1.75664775  0.27588598 -0.94141758 -1.28359555]]\n",
      "Syn0 direction changes\n",
      "[[ 0.  3.  1.  0.]\n",
      " [ 1.  2.  1.  1.]\n",
      " [ 1.  3.  2.  1.]]\n",
      "Syn1\n",
      "[[-4.35924342]\n",
      " [-8.98285631]\n",
      " [ 8.85386503]\n",
      " [ 5.72451831]]\n",
      "Syn1 direction changes\n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "Training with α 1\n",
      "Error: 0.498898932827\n",
      "Error: 0.0106626404486\n",
      "Error: 0.0072905803823\n",
      "Error: 0.00586982820003\n",
      "Error: 0.00504180776074\n",
      "Error: 0.0044844028728\n",
      "Syn0\n",
      "[[ 6.29389485 -3.93014031  5.83768059 -6.07775283]\n",
      " [-4.27118396  7.55458043  5.11489077  3.67841934]\n",
      " [ 1.7265102   0.50022269 -0.94295592 -1.2600877 ]]\n",
      "Syn0 direction changes\n",
      "[[ 0.  3.  1.  0.]\n",
      " [ 1.  2.  1.  1.]\n",
      " [ 1.  3.  3.  1.]]\n",
      "Syn1\n",
      "[[ -4.81026542]\n",
      " [-11.30791486]\n",
      " [ 10.46331936]\n",
      " [  7.50856908]]\n",
      "Syn1 direction changes\n",
      "[[ 1.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "Training with α 10\n",
      "Error: 0.498898932827\n",
      "Error: 0.00277424125556\n",
      "Error: 0.00194289672755\n",
      "Error: 0.00157880649911\n",
      "Error: 0.00136305538713\n",
      "Error: 0.00121639933639\n",
      "Syn0\n",
      "[[ 4.58765922 -3.78982141 -3.85579636 -8.16356415]\n",
      " [-6.13992351 -3.41795688  5.6952012   7.19195381]\n",
      " [-2.23537992  0.41868563  1.77695382 -3.47480627]]\n",
      "Syn0 direction changes\n",
      "[[ 12.  13.  18.  10.]\n",
      " [ 17.  19.  11.  21.]\n",
      " [ 23.  19.  27.  30.]]\n",
      "Syn1\n",
      "[[  8.58620468]\n",
      " [ -4.11132486]\n",
      " [ -7.0593988 ]\n",
      " [ 14.24778216]]\n",
      "Syn1 direction changes\n",
      "[[ 48.]\n",
      " [ 30.]\n",
      " [ 31.]\n",
      " [ 25.]]\n",
      "Training with α 100\n",
      "Error: 0.498898932827\n",
      "Error: 0.37602327484\n",
      "Error: 0.375140780506\n",
      "Error: 0.250379755933\n",
      "Error: 0.250283743662\n",
      "Error: 0.250237069203\n",
      "Syn0\n",
      "[[ -3.12298022 -16.62574909  -2.51286331 -16.94594301]\n",
      " [-16.17800615 -10.69899139 -10.11405424  14.56127307]\n",
      " [-17.72416446   1.35623773 -14.03769859 -13.1169503 ]]\n",
      "Syn0 direction changes\n",
      "[[  4.   3.   6.  10.]\n",
      " [  7.   5.   3.   5.]\n",
      " [  4.   7.   6.   7.]]\n",
      "Syn1\n",
      "[[ 1.85603088]\n",
      " [-9.72655018]\n",
      " [-3.28296007]\n",
      " [ 9.69250425]]\n",
      "Syn1 direction changes\n",
      "[[ 6.]\n",
      " [ 4.]\n",
      " [ 8.]\n",
      " [ 7.]]\n",
      "Training with α 1000\n",
      "Error: 0.498898932827\n",
      "Error: 0.5\n",
      "Error: 0.5\n",
      "Error: 0.5\n",
      "Error: 0.5\n",
      "Error: 0.5\n",
      "Syn0\n",
      "[[  2.34398892  -0.40565517 -15.36651341  -0.74559321]\n",
      " [  0.72362831   0.2201034  -14.84941188  -1.02270317]\n",
      " [ -0.2828915   -0.73850692 -34.25532461  -3.28574944]]\n",
      "Syn0 direction changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 1.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]]\n",
      "Syn1\n",
      "[[-52.45494335]\n",
      " [-18.76257439]\n",
      " [-13.64165693]\n",
      " [ -1.41855461]]\n",
      "Syn1 direction changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Output: expected classifications (4 x 1)\n",
    "nnOutput3 = np.array([[0, 1, 1 ,0]]).T\n",
    "\n",
    "alphas = [.001, .01, .1, 1, 10, 100, 1000]\n",
    "\n",
    "# Try training with each different alpha-value\n",
    "for a in alphas:\n",
    "    # Initialize weights with mean of 0, seed random generator\n",
    "    np.random.seed(271015)\n",
    "    syn0 = 2 * np.random.random((3, 4)) - 1  # (3 x 4)\n",
    "    syn1 = 2 * np.random.random((4, 1)) - 1  # (4 x 1)\n",
    "\n",
    "    # This time, we track the previous weights for syn0 and syn1, as well as the direction of change\n",
    "    syn0prev = np.zeros_like(syn0)\n",
    "    syn1prev = np.zeros_like(syn1)\n",
    "    syn0dir = np.zeros_like(syn0)\n",
    "    syn1dir = np.zeros_like(syn1)\n",
    "    \n",
    "    print(\"Training with \\u03B1 \" + str(a))  # 03B1 is unicode for lowercase alpha\n",
    "    \n",
    "    # Do full-batch learning\n",
    "    for i in range(60000):\n",
    "\n",
    "        # Forward Propagation\n",
    "        l0 = nnInput\n",
    "        l1 = nonlin(l0 @ syn0)\n",
    "        l2 = nonlin(l1 @ syn1)\n",
    "\n",
    "        # Output error: expected minus predicted (4 x 1)\n",
    "        errl2 = nnOutput3 - l2\n",
    "\n",
    "        # Getting a running view of the error every 10000 steps\n",
    "        if not i % 10000:\n",
    "            print(\"Error: \" + str(np.mean(np.abs(errl2))))\n",
    "\n",
    "        # Calculate delta for output layer: the error times slope of the sigmoid at l2 values (4 x 1)\n",
    "        deltal2 = errl2 * nonlin(l2, deriv=True)\n",
    "\n",
    "        # Hidden error: how much hidden layer contributed to output error (4 x 4)\n",
    "        # THIS IS THE BACK-PROPAGATION STEP\n",
    "        errl1 = deltal2 @ syn1.T\n",
    "\n",
    "        # Calculate delta for hidden layer: the error times the sigmoid at l2 values (4 x 4)\n",
    "        deltal1 = errl1 * nonlin(l1, deriv=True)\n",
    "\n",
    "        # Calculate the updates to the weights\n",
    "        syn0update = l0.T @ deltal1\n",
    "        syn1update = l1.T @ deltal2\n",
    "        \n",
    "        # For tracking, we see if the direction of the update has changed since the last step \n",
    "        syn0dir += (syn0update * syn0prev) < 0\n",
    "        syn1dir += (syn1update * syn1prev) < 0\n",
    "        \n",
    "        # Update weights: this is where the alpha values come in\n",
    "        syn0 += a * syn0update\n",
    "        syn1 += a * syn1update\n",
    "        \n",
    "        syn0prev = syn0update\n",
    "        syn1prev = syn1update\n",
    "        \n",
    "    # Print out the results for this alpha\n",
    "    print(\"Syn0\")\n",
    "    print(syn0)\n",
    "    print(\"Syn0 direction changes\")\n",
    "    print(syn0dir)\n",
    "\n",
    "    print(\"Syn1\")\n",
    "    print(syn1)\n",
    "    print(\"Syn1 direction changes\")\n",
    "    print(syn1dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer back-propagating network Mk. 3: Parameterizing the hidden layer size\n",
    "\n",
    "For this dataset, the previous excercise demonstrated that an alpha of 10 worked best, and the hidden layer size was fixed at 4 x 1. In the original iamtrask post, this step only tested the performance of a network with 32 nodes. Instead of testing just one size, we'll try several here and keep the alpha constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2 hidden layer nodes \n",
      "Error: 0.500542514772\n",
      "Error: 0.375678086516\n",
      "Error: 0.375611385993\n",
      "Error: 0.375562977412\n",
      "Error: 0.375527817871\n",
      "Error: 0.375502700424\n",
      "Training with 4 hidden layer nodes \n",
      "Error: 0.498898932827\n",
      "Error: 0.00277424125556\n",
      "Error: 0.00194289672755\n",
      "Error: 0.00157880649911\n",
      "Error: 0.00136305538713\n",
      "Error: 0.00121639933639\n",
      "Training with 8 hidden layer nodes \n",
      "Error: 0.498573920102\n",
      "Error: 0.00247201806838\n",
      "Error: 0.00170567040317\n",
      "Error: 0.00137593796584\n",
      "Error: 0.00118242179508\n",
      "Error: 0.00105172558577\n",
      "Training with 16 hidden layer nodes \n",
      "Error: 0.496319067934\n",
      "Error: 0.00230574062297\n",
      "Error: 0.00159283504815\n",
      "Error: 0.00128457712047\n",
      "Error: 0.0011032857294\n",
      "Error: 0.000980712209009\n",
      "Training with 32 hidden layer nodes \n",
      "Error: 0.493817543054\n",
      "Error: 0.00332116672913\n",
      "Error: 0.0018904703995\n",
      "Error: 0.00144658328493\n",
      "Error: 0.00121176944923\n",
      "Error: 0.00106133852859\n",
      "Training with 64 hidden layer nodes \n",
      "Error: 0.495697784403\n",
      "Error: 0.499999999843\n",
      "Error: 0.499999999842\n",
      "Error: 0.499999999842\n",
      "Error: 0.499999999841\n",
      "Error: 0.499999999841\n"
     ]
    }
   ],
   "source": [
    "# Output: expected classifications (4 x 1)\n",
    "nnOutput3 = np.array([[0, 1, 1 ,0]]).T\n",
    "\n",
    "# Set alpha and the hidden layer sizes we want to test\n",
    "a = 10\n",
    "l1sizes = [2, 4, 8, 16, 32, 64]\n",
    "\n",
    "# Try training with each different alpha-value\n",
    "for s in l1sizes:\n",
    "    # Initialize weights with mean of 0, seed random generator\n",
    "    # This is where the different hidden layer sizes come in\n",
    "    np.random.seed(271015)\n",
    "    syn0 = 2 * np.random.random((3, s)) - 1  # (3 x s)\n",
    "    syn1 = 2 * np.random.random((s, 1)) - 1  # (s x 1)\n",
    "    \n",
    "    print(\"Training with {} hidden layer nodes \".format(s))\n",
    "    \n",
    "    # Do full-batch learning\n",
    "    for i in range(60000):\n",
    "\n",
    "        # Forward Propagation\n",
    "        l0 = nnInput\n",
    "        l1 = nonlin(l0 @ syn0)\n",
    "        l2 = nonlin(l1 @ syn1)\n",
    "\n",
    "        # Output error: expected minus predicted (4 x 1)\n",
    "        errl2 = nnOutput3 - l2\n",
    "\n",
    "        # Getting a running view of the error every 10000 steps\n",
    "        if not i % 10000:\n",
    "            print(\"Error: \" + str(np.mean(np.abs(errl2))))\n",
    "\n",
    "        # Calculate delta for output layer: the error times slope of the sigmoid at l2 values (4 x 1)\n",
    "        deltal2 = errl2 * nonlin(l2, deriv=True)\n",
    "\n",
    "        # Hidden error: how much hidden layer contributed to output error (4 x 4)\n",
    "        # THIS IS THE BACK-PROPAGATION STEP\n",
    "        errl1 = deltal2 @ syn1.T\n",
    "\n",
    "        # Calculate delta for hidden layer: the error times the sigmoid at l2 values (4 x 4)\n",
    "        deltal1 = errl1 * nonlin(l1, deriv=True)\n",
    "\n",
    "        # Calculate the updates to the weights\n",
    "        syn0 += a * (l0.T @ deltal1)\n",
    "        syn1 += a * (l1.T @ deltal2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-layer back-propagating network with Hinton's Dropout\n",
    "\n",
    "The above simulations suggest our network works best with an alpha of 10 and a hidden layer size of 16 nodes. The network with 32 nodes doesn't do significantly worse, so we'll use that in the simulations below, so that our hidden layer is significantly larger than either the input or output layer. We'll see how training does with dropout rates of 10-50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 10% hidden layer dropout \n",
      "Error: 0.528716579972\n",
      "Error: 2.20441872404e-19\n",
      "Error: 1.54321083553e-14\n",
      "Error: 1.91232415421e-18\n",
      "Error: 7.02243940894e-25\n",
      "Error: 2.57849416973e-30\n",
      "Training with 20% hidden layer dropout \n",
      "Error: 0.539856694952\n",
      "Error: 6.12735409176e-23\n",
      "Error: 1.50692407773e-40\n",
      "Error: 3.85085218196e-50\n",
      "Error: 3.0852487622e-35\n",
      "Error: 1.26822046033e-33\n",
      "Training with 30% hidden layer dropout \n",
      "Error: 0.510231292419\n",
      "Error: 0.500005334237\n",
      "Error: 2.39094863462e-42\n",
      "Error: 9.87170454361e-82\n",
      "Error: 8.31397762445e-77\n",
      "Error: 5.59973592673e-75\n",
      "Training with 40% hidden layer dropout \n",
      "Error: 0.497345959473\n",
      "Error: 5.1089134684e-70\n",
      "Error: 3.54407439107e-114\n",
      "Error: 0.25\n",
      "Error: 1.59354836775e-124\n",
      "Error: 8.95111618158e-184\n",
      "Training with 50% hidden layer dropout \n",
      "Error: 0.550690223828\n",
      "Error: 8.24443880427e-103\n",
      "Error: 2.0994128787e-112\n",
      "Error: 1.66267702286e-207\n",
      "Error: 0.0\n",
      "Error: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/IPython/kernel/__main__.py:5: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "# Output: expected classifications (4 x 1)\n",
    "nnOutput3 = np.array([[0, 1, 1 ,0]]).T\n",
    "\n",
    "# Set alpha and the hidden layer dropout rates\n",
    "a = 10\n",
    "l1size = 32\n",
    "dropouts = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Try training with each different alpha-value\n",
    "for d in dropouts:\n",
    "    # Initialize weights with mean of 0, seed random generator\n",
    "    # This is where the different hidden layer sizes come in\n",
    "    np.random.seed(13)  # I changed the seed here for reasons explained in the next cell\n",
    "    syn0 = 2 * np.random.random((3, l1size)) - 1  # (3 x 32)\n",
    "    syn1 = 2 * np.random.random((l1size, 1)) - 1  # (32 x 1)\n",
    "    \n",
    "    print(\"Training with {}% hidden layer dropout \".format(int(100*d)))\n",
    "    \n",
    "    # Do full-batch learning\n",
    "    for i in range(60000):\n",
    "\n",
    "        # Forward Propagation\n",
    "        l0 = nnInput\n",
    "        l1 = nonlin(l0 @ syn0)\n",
    "        \n",
    "        # DROPOUT: Note the division by (1-d) to compensate for the dropped node activity\n",
    "        l1dropped = l1 * np.random.binomial(1, 1.0-d, size=l1.shape) / (1.0 - d)\n",
    "        l2 = nonlin(l1dropped @ syn1)\n",
    "\n",
    "        # Output error: expected minus predicted (4 x 1)\n",
    "        errl2 = nnOutput3 - l2\n",
    "\n",
    "        # Getting a running view of the error every 10000 \n",
    "        if not i % 10000:\n",
    "            print(\"Error: \" + str(np.mean(np.abs(errl2))))\n",
    "\n",
    "        # Calculate delta for output layer: the error times slope of the sigmoid at l2 values (4 x 1)\n",
    "        deltal2 = errl2 * nonlin(l2, deriv=True)\n",
    "\n",
    "        # Hidden error: how much hidden layer contributed to output error (4 x 4)\n",
    "        # THIS IS THE BACK-PROPAGATION STEP\n",
    "        errl1 = deltal2 @ syn1.T\n",
    "\n",
    "        # Calculate delta for hidden layer: the error times the sigmoid at l2 values (4 x 4)\n",
    "        # In back-propagation, we do NOT drop nodes\n",
    "        deltal1 = errl1 * nonlin(l1, deriv=True)\n",
    "\n",
    "        # Calculate the updates to the weights\n",
    "        syn0 += a * (l0.T @ deltal1)\n",
    "        syn1 += a * (l1.T @ deltal2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some interesting results. Dropout rates of 20-30% seem to push the network error astronomically low in just a few iterations. However, higher or lower dropout rates seem to reduce the performance of the network. This may be because the hidden layer is relatively small. When the number of hidden nodes is increased to 32, this effect disappears and all dropout rates result in networks performing much better than any of the networks without dropout.\n",
    "\n",
    "Note: Some seeds seem to make networks stall in improvement (usually at 0.25 or 0.50 error). Changing the seed has a dramatic effect on this, so I think it's a problem with the setup of the problem or an implementation problem in the binomial distribution, not the dropout procedure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
